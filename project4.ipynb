{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "np.warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# set Jupyter to display ALL output from a cell (not just last output)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" \n",
    "\n",
    "# set pandas and numpy options to make print format nicer\n",
    "pd.set_option(\"display.width\",100)\n",
    "pd.set_option(\"display.max_columns\",100)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "np.set_printoptions(linewidth=120, threshold=5000, edgeitems=50, suppress=True)\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Loans csv and Create test/train csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: data.tsv into revs dataframe...\n",
      "revs dataframe: (50000, 3)\n",
      "ids dataframe: (25000, 3)\n",
      "Split 1 (25000, 3) (25000, 2)\n",
      "Split 2 (25000, 3) (25000, 2)\n",
      "Split 3 (25000, 3) (25000, 2)\n",
      "Writing train, test, labels csv files...\n",
      "Files Saved\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=seed)\n",
    "\n",
    "print('reading: data.tsv into revs dataframe...')\n",
    "revs = pd.read_csv('data.tsv', sep=' ', quotechar='\"', escapechar='\\\\')\n",
    "print('revs dataframe:', revs.shape)\n",
    "\n",
    "splits = pd.read_csv('splits.csv',sep='\\t', dtype={'split_1':int,'split_2':int, 'split_3':int,})\n",
    "print('ids dataframe:', splits.shape)\n",
    "\n",
    "trains = []\n",
    "tests = []\n",
    "labels = revs[['new_id','sentiment']]\n",
    "for i, col in enumerate(splits.columns):\n",
    "    trains.append(revs.loc[~revs.new_id.isin(splits[col]),:])\n",
    "    tests.append( revs.loc[ revs.new_id.isin(splits[col]), revs.columns!='sentiment'])\n",
    "    print('Split', i+1, trains[i].shape, tests[i].shape)\n",
    "\n",
    "print('Writing train, test, labels csv files...')\n",
    "fold=0\n",
    "_ = trains[fold].to_csv('train.csv', index=False)\n",
    "_ = tests [fold].to_csv('test.csv',  index=False)\n",
    "print('Files Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_train_test(train, test):    \n",
    "    train['review'] = train.review.str.replace('<br /><br />',' ')\n",
    "    test ['review'] =  test.review.str.replace('<br /><br />',' ')\n",
    "\n",
    "    stop_words=['the','with','he','she','also','made','had','out','in','his','hers','there','was','then'] \n",
    "\n",
    "    cv = TfidfVectorizer(stop_words=stop_words, ngram_range=(1,2), min_df=20, max_df=0.3)\n",
    "    X_train = cv.fit_transform(train.review).toarray()\n",
    "    X_test  = cv.transform(test.review).toarray()\n",
    "    \n",
    "    y_train = train.sentiment\n",
    "    vocab = np.array(cv.get_feature_names())\n",
    "    return X_train, y_train, X_test, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocab.txt and word_weights.csv files from t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 55.6 s, total: 2min 20s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "split = 0\n",
    "X_train, y_train, X_test, vocab = prep_train_test(revs.copy(), revs.copy())\n",
    "\n",
    "t_test = ttest_ind(X_train[y_train==1, :], X_train[y_train==0, :])\n",
    "\n",
    "voc_df = pd.DataFrame({'tstat': t_test.statistic, 'word': vocab})\n",
    "voc_df['magn_tstat'] = voc_df.tstat.abs()\n",
    "voc_df = voc_df.sort_values('magn_tstat',ascending=False)\n",
    "\n",
    "voc_df = voc_df.head(2900)\n",
    "voc_df['weight'] = np.power((voc_df.magn_tstat - voc_df.magn_tstat.min()), 1.2)\n",
    "voc_df['weight'] = (voc_df['weight'] / voc_df.weight.max() * 21 * np.sign(voc_df.tstat)).round(4)\n",
    "\n",
    "voc_df[['word','weight']].to_csv('word_weights.csv', index=False)\n",
    "np.savetxt('vocab.txt',voc_df.word.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models using vocab files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2, Split:0, AUC:0.96685, Vocab:2889, RunTime: 30.57 secs\n",
      "Created Result_1.txt, rows= 25000\n",
      "L2, Split:1, AUC:0.9658 , Vocab:2890, RunTime: 30.01 secs\n",
      "Created Result_2.txt, rows= 25000\n",
      "L2, Split:2, AUC:0.96612, Vocab:2888, RunTime: 30.50 secs\n",
      "Created Result_3.txt, rows= 25000\n"
     ]
    }
   ],
   "source": [
    "vocab_slim = np.loadtxt('vocab.txt', dtype=np.str, delimiter='\\n')\n",
    "\n",
    "num_folds = len(splits.columns)\n",
    "for fold in range(num_folds):\n",
    "    start_time = time.time()\n",
    "    X_train, y_train, X_test, vocab = prep_train_test(trains[fold].copy(), tests[fold].copy())\n",
    "    y_test = pd.merge(tests[fold][['new_id']], labels, how='left', on='new_id')\n",
    "\n",
    "    indices = np.where(np.in1d(vocab, vocab_slim))[0]\n",
    "    X_train = X_train[:, indices].copy()\n",
    "    X_test  = X_test [:, indices].copy()\n",
    "    \n",
    "    model = LogisticRegression(penalty='l2',C=17, random_state=seed)\n",
    "    _ = model.fit(X_train, y_train)\n",
    "    probs = model.predict_proba(X_test)[:,1]\n",
    "    print('L2, Split:{}, AUC:{:<7.5}, Vocab:{}, RunTime:{:6.2f} secs'.format(\n",
    "        fold, round(roc_auc_score(y_test.sentiment, probs),5), X_train.shape[1], round(time.time()-start_time,2))) \n",
    "    df = pd.DataFrame({'new_id': tests[fold].new_id, 'prob': probs.round(5)})\n",
    "    df.to_csv('Result_'+str(fold+1)+'.txt', index=False)\n",
    "    print('Created Result_'+str(fold+1)+'.txt, rows=', df.shape[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Submission File (generated from mymain.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model AUC: 0.96612\n",
      "model AUC: 0.96685\n",
      "model AUC: 0.9658\n",
      "model AUC: 0.96612\n"
     ]
    }
   ],
   "source": [
    "filenames = ['mysubmission.txt','Result_1.txt', 'Result_2.txt', 'Result_3.txt']\n",
    "for filename in filenames:\n",
    "    res = pd.read_csv(filename)\n",
    "    y_test = pd.merge(res[['new_id']], labels, how='left', on='new_id')\n",
    "    print('model AUC:', round(roc_auc_score(y_test.sentiment, res.prob),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 1200\n",
      "L2, Split:0, AUC:0.96163, Vocab:1199, RunTime: 27.54 secs\n",
      "L2, Split:1, AUC:0.96014, Vocab:1200, RunTime: 27.24 secs\n",
      "L2, Split:2, AUC:0.96069, Vocab:1200, RunTime: 26.71 secs\n",
      "Vocab: 2900\n",
      "L2, Split:0, AUC:0.96685, Vocab:2889, RunTime: 31.68 secs\n",
      "L2, Split:1, AUC:0.9658 , Vocab:2890, RunTime: 29.71 secs\n",
      "L2, Split:2, AUC:0.96612, Vocab:2888, RunTime: 29.53 secs\n",
      "Vocab: 2999\n",
      "L2, Split:0, AUC:0.96677, Vocab:2987, RunTime: 31.15 secs\n",
      "L2, Split:1, AUC:0.96587, Vocab:2987, RunTime: 29.50 secs\n",
      "L2, Split:2, AUC:0.96616, Vocab:2985, RunTime: 29.72 secs\n"
     ]
    }
   ],
   "source": [
    "for vocab_size in [1200, 2900, 2999]:\n",
    "    print('Vocab:', vocab_size)\n",
    "    split = 0\n",
    "    X_train, y_train, X_test, vocab = prep_train_test(revs.copy(), revs.copy())\n",
    "\n",
    "    t_test = ttest_ind(X_train[y_train==1, :], X_train[y_train==0, :])\n",
    "\n",
    "    voc_df = pd.DataFrame({'tstat': t_test.statistic, 'word': vocab})\n",
    "    voc_df['magn_tstat'] = voc_df.tstat.abs()\n",
    "    voc_df = voc_df.sort_values('magn_tstat',ascending=False)\n",
    "\n",
    "    voc_df = voc_df.head(vocab_size)\n",
    "    voc_df['weight'] = np.power((voc_df.magn_tstat - voc_df.magn_tstat.min()), 1.2)\n",
    "    voc_df['weight'] = (voc_df['weight'] / voc_df.weight.max() * 21 * np.sign(voc_df.tstat)).round(4)\n",
    "\n",
    "    voc_df[['word','weight']].to_csv('word_weights.csv', index=False)\n",
    "    np.savetxt('vocab.txt',voc_df.word.values, fmt='%s')\n",
    "    vocab_slim = np.loadtxt('vocab.txt', dtype=np.str, delimiter='\\n')\n",
    "\n",
    "    num_folds = len(splits.columns)\n",
    "    for fold in range(num_folds):\n",
    "        start_time = time.time()\n",
    "        X_train, y_train, X_test, vocab = prep_train_test(trains[fold].copy(), tests[fold].copy())\n",
    "        y_test = pd.merge(tests[fold][['new_id']], labels, how='left', on='new_id')\n",
    "\n",
    "        indices = np.where(np.in1d(vocab, vocab_slim))[0]\n",
    "        X_train = X_train[:, indices].copy()\n",
    "        X_test  = X_test [:, indices].copy()\n",
    "\n",
    "        model = LogisticRegression(penalty='l2',C=17, random_state=seed)\n",
    "        _ = model.fit(X_train, y_train)\n",
    "        probs = model.predict_proba(X_test)[:,1]\n",
    "        print('L2, Split:{}, AUC:{:<7.5}, Vocab:{}, RunTime:{:6.2f} secs'.format(\n",
    "            fold, round(roc_auc_score(y_test.sentiment, probs),5), X_train.shape[1], round(time.time()-start_time,2))) \n",
    "        df = pd.DataFrame({'new_id': tests[fold].new_id, 'prob': probs.round(5)})\n",
    "        df.to_csv('Result_'+str(fold+1)+'.txt', index=False)\n",
    "#         print('Created Result_'+str(fold+1)+'.txt, rows=', df.shape[0]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
